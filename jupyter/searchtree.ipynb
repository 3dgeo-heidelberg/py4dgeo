{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db1cd31f-dd6f-4219-9a79-f9ef34821ffd",
   "metadata": {},
   "source": [
    "# Search Tree Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fbca9a-5a3a-41a4-802e-7237678c3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py4dgeo\n",
    "import numpy as np\n",
    "from scipy.spatial import KDTree as ScipyKDTree\n",
    "from sklearn.neighbors import KDTree as SklearnKDTree\n",
    "import laspy\n",
    "from time import perf_counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cf1924-da59-498c-a697-aa863e4cc1eb",
   "metadata": {},
   "source": [
    "The measurement is carried out by decorating functions with the following decorator which will measure the execution time and return it as a tuple with the original return value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f833fc85-008a-417b-8c07-4318d44966d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(f):\n",
    "    \"\"\" A decorator that measures execution time and returns it as part of a tuple \"\"\"\n",
    "    def _decorated(*args, **kwargs):\n",
    "        start = perf_counter()\n",
    "        ret = f(*args, **kwargs)\n",
    "        return perf_counter() - start, ret\n",
    "    return _decorated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d46ea-d5fd-425d-9027-38c2614e2839",
   "metadata": {},
   "source": [
    "To account for noise, small execution times etc., we repeat the function execution a number of times and take the minimum across these runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0eaf68-d4b6-4bc2-88b9-5f5078eef224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum_across_runs(n, func, *args):\n",
    "    measurements = []\n",
    "    for _ in range(n):\n",
    "        t, result = func(*args)\n",
    "        measurements.append(t)\n",
    "    return min(measurements), result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bb24df-4ded-44fd-8436-c21879fdf075",
   "metadata": {},
   "source": [
    "Our first test is a point cloud of randomly distributed points in the unit cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63498587-dad4-45ae-817b-56d1bcbc5435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_data(n):\n",
    "    \"\"\" Create n samples within the unitcube \"\"\"\n",
    "    rng = np.random.default_rng()\n",
    "    return rng.uniform([0, 0, 0], [1, 1, 1], size=(n, 3)).astype('f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f63905-4d1a-4344-b999-ea048c46fd56",
   "metadata": {},
   "source": [
    "## Point Cloud Library (PCL)\n",
    "\n",
    "PCL provides a module `pcl::search` that contains several implementations of a unified search interface: KDTree, OCTree, Bruteforce. The KDTree implementation uses the library FLANN. A general problem of PCL, that it uses a padded data structure for a 3D point to be SSE-friendly (allocating `float[4]`). This hinders seemless interoperability with `numpy.array` as the memory can only be shared if the input data uses that same layout. Currently, the constructor of `py4dgeo.PCLPointCloud` makes a copy of the point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42785903-b77f-4d17-ad4b-84bf05247e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure\n",
    "def build_pcl_kdtree(data):\n",
    "    \"\"\" Build PCL KDTree data structure. \"\"\"\n",
    "    pc = py4dgeo.PCLPointCloud(data)\n",
    "    pc.build_tree(py4dgeo.SearchStrategy.kdtree)\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2172ead-0a22-49a7-9236-803d1f6bf7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure\n",
    "def build_pcl_bruteforce(data):\n",
    "    \"\"\" Build PCL Bruteforce data structure. \"\"\"\n",
    "    pc = py4dgeo.PCLPointCloud(data)\n",
    "    pc.build_tree(py4dgeo.SearchStrategy.bruteforce)\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de2f9a-0d33-4f7a-b69f-906cbe042911",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure\n",
    "def build_pcl_octree(data):\n",
    "    \"\"\" Build PCL OCTree data structure. \"\"\"\n",
    "    pc = py4dgeo.PCLPointCloud(data)\n",
    "    pc.build_tree(py4dgeo.SearchStrategy.octree)\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ec11c-9eea-4aa5-9a09-94c16b48759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure\n",
    "def radius_mine(tree, point, radius):\n",
    "    \"\"\" Invocation of radius search for PCL trees \"\"\"\n",
    "    return tree.radius_search(point, radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e497d31-a8e5-498b-8298-74dcc20c10bb",
   "metadata": {},
   "source": [
    "## NanoFLANN\n",
    "\n",
    "NanoFLANN is a fork of the original FLANN library. Among the reasons to fork, there are several which are beneficial to our use case:\n",
    "\n",
    "* Single header with permissive license -> Just copy into your project\n",
    "* Performance gains by removing abstractions\n",
    "* No approximate searchs (we do not need them?)\n",
    "* No rigid assumptions on data layout -> Easy to integrate copyless\n",
    "\n",
    "NanoFLANN is also the search tree library used by PDAL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eac84b-135d-4de6-8bd4-2e50572d6909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nanoflann(param=10):\n",
    "    @measure\n",
    "    def _build_nanoflann(data):\n",
    "        pc = py4dgeo.NFPointCloud2(data)\n",
    "        pc.build_tree(param)\n",
    "        return pc\n",
    "    return _build_nanoflann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196837a5-1053-41c4-a0ec-5e0f1252ff2b",
   "metadata": {},
   "source": [
    "## CloudCompare Core library\n",
    "\n",
    "I originally promised a reference implementation with [CloudCompare's core library](https://github.com/CloudCompare/CCCoreLib). However, while looking at the library, I turned away for several reasons:\n",
    "\n",
    "* There is no documentation.\n",
    "* There is not a single test.\n",
    "* The PointCloud data structure is quite abstraction-heavy and does not allow copy-less integration with `numpy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8888dbc-1209-48be-a469-559f2e2aadb0",
   "metadata": {},
   "source": [
    "## Python reference implementations\n",
    "\n",
    "We use the following two reference implementations: `scipy` and `scikit-learn`. `scipy` has a custom C++ implementation of a KDTree, `scikit-learn` uses Cython. I suspect the latter to be optimized for high dimensional problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a163b-7183-4c87-9616-699cb1eefcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scipy(param=10):\n",
    "    @measure\n",
    "    def _build_scipy(data):\n",
    "        \"\"\" Build Scipy KDTree data structure \"\"\"\n",
    "        return ScipyKDTree(data, leafsize=param)\n",
    "    return _build_scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4240d-e198-407b-baa9-c0dcc5b856a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure\n",
    "def radius_scipy(tree, point, radius):\n",
    "    \"\"\" Invocation of Scipy radius search \"\"\"\n",
    "    return tree.query_ball_point(point, radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b54bf-796c-4018-b657-b4c9ec82222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure\n",
    "def build_sklearn(data):\n",
    "    \"\"\" Build Sklearn KDTree data structure \"\"\"\n",
    "    return SklearnKDTree(data, leaf_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d069c1ce-ba15-40b5-892d-2a6000493a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure\n",
    "def radius_sklearn(tree, point, radius):\n",
    "    \"\"\" Invocation of Sklearn radius search \"\"\"\n",
    "    return tree.query_radius(np.expand_dims(point, axis=0), radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5d8998-ff4d-459c-b77d-f30d132de796",
   "metadata": {},
   "source": [
    "## General comparison\n",
    "\n",
    "The general methodology for the comparison is the following: For an increasing number of points `n` in the point cloud, measure the build time and the query execution time for a radius search individually. We report the KDTree build time as seconds per point in the point cloud. For the radius query, we construct the search radius such that the number of points in the return set stays constant for varying `n` (of course this only holds in a stochastical sense). Consequently, we report the absolute execution time of the query. As the libraries do not provide an interface for performing multiple queries, benchmarking a single query seems okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed0ba4-e40c-4fa1-926e-01fab5717d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(impls, max_n=10, radius_scale=2.0):\n",
    "    nsamples = [2**i * 1000 for i in range(max_n)]\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    for name, build_func, radius_func in impls:\n",
    "        build_times = []\n",
    "        query_times = []\n",
    "        for n in nsamples:\n",
    "            data = create_random_data(n)\n",
    "            build_time, cloud = minimum_across_runs(10, build_func, data)\n",
    "            build_times.append(build_time / n)\n",
    "            query_time, result = minimum_across_runs(10, radius_func, cloud, np.array([0.5, 0.5, 0.5]), radius_scale * n ** (-(1/3)))\n",
    "            query_times.append(query_time)\n",
    "        axs[0].plot(nsamples, build_times, label=name)\n",
    "        axs[1].plot(nsamples, query_times, label=name)\n",
    "    axs[0].set_xscale(\"log\")\n",
    "    axs[1].set_xscale(\"log\")\n",
    "    axs[0].set_xlabel(\"Point Cloud size\")\n",
    "    axs[0].set_ylabel(\"Time/Point [s]\")\n",
    "    axs[1].set_xlabel(\"Point Cloud size\")\n",
    "    axs[1].set_ylabel(\"Query Time [s]\")\n",
    "    axs[0].set_title(\"KDTree build times/point\")\n",
    "    axs[1].set_title(\"KDTree query time (appr. constant return size)\")\n",
    "    axs[0].legend()\n",
    "    axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ae4d48-c88e-45f9-ae28-6b942e9016ad",
   "metadata": {},
   "source": [
    "This is the comparison of the \"off-the-shelf\" versions of the libraries, where we can clearly see that `SKLearn` is quite a bit slower both for building a querying. This might of course be related to it being optimized for high-dimensionality applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0757f0-c87c-40ba-8530-db4d824e64be",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare([\n",
    "    (\"PCL KDTree\", build_pcl_kdtree, radius_mine),\n",
    "    (\"SciPy KDTree\", build_scipy(), radius_scipy),\n",
    "    (\"NanoFLANN\", build_nanoflann(), radius_mine),\n",
    "    (\"SKLearn KDTree\", build_sklearn, radius_sklearn),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8bda3b-1fa3-4bae-bfab-b7c0d6b71b42",
   "metadata": {},
   "source": [
    "Looking at the other PCL variants (which were 0 overhead to implement due to the unified interface), we can clearly see that using a KDTree is absolutely mandatory, but we knew that already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8206f7-0c54-42be-aeb7-98203d20e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare([\n",
    "    (\"PCL KDTree\", build_pcl_kdtree, radius_mine),\n",
    "    (\"PCL Bruteforce\", build_pcl_bruteforce, radius_mine),\n",
    "    (\"PCL OCtree\", build_pcl_octree, radius_mine),\n",
    "], max_n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67317b00-d6b8-49d8-9ccc-d883fd2fbb10",
   "metadata": {},
   "source": [
    "Returning to the top contenders, we can see that the performance result actually changes with the search radius (and with it the number of returned points):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fdd6ec-d670-46df-b328-3a38e617360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare([\n",
    "    (\"PCL KDTree\", build_pcl_kdtree, radius_mine),\n",
    "    (\"SciPy KDTree\", build_scipy(), radius_scipy),\n",
    "    (\"NanoFLANN\", build_nanoflann(), radius_mine),\n",
    "], radius_scale=4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d339d021-a05c-4092-b2f4-2ada165662a4",
   "metadata": {},
   "source": [
    "Actually, at some point SciPy has the fastest off-the-shelf implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128d381-6eaa-42e3-910c-58759ef49c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare([\n",
    "    (\"PCL KDTree\", build_pcl_kdtree, radius_mine),\n",
    "    (\"SciPy KDTree\", build_scipy(), radius_scipy),\n",
    "    (\"NanoFLANN\", build_nanoflann(), radius_mine),\n",
    "], radius_scale=8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd80071-cffc-4a8b-906b-2aea556f2d53",
   "metadata": {},
   "source": [
    "## Parameter Tuning\n",
    "\n",
    "It is worth taking a close look at the parameter tuning for these trees. The most prominent tuning parameter that balances build time vs. query time is the cutoff at which the implementation switches over to a bruteforce algorithm. The implications can be seen e.g. for NanoFLANN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad3f7ac-c18c-4d07-bc2f-39919437f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare([\n",
    "    (\"NanoFLANN 5\", build_nanoflann(5), radius_mine),\n",
    "    (\"NanoFLANN 10\", build_nanoflann(10), radius_mine),\n",
    "    (\"NanoFLANN 20\", build_nanoflann(20), radius_mine),\n",
    "    (\"NanoFLANN 50\", build_nanoflann(50), radius_mine),\n",
    "    (\"NanoFLANN 100\", build_nanoflann(100), radius_mine),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa2e056-56cc-4cd7-82c1-a0d543590094",
   "metadata": {},
   "source": [
    "A similar study can be done with SciPy, where we see that the qualitative behaviour is similar, but the absolute build times of NanoFLANN are faster in general. This might be related to additional trade-off decisions introduced by the `balanced_tree` and `compact_nodes` parameters of `ScipyKDTree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4faaa3-8d60-45bc-8510-8b193ba94007",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare([\n",
    "    (\"SciPy 5\", build_scipy(5), radius_scipy),\n",
    "    (\"SciPy 10\", build_scipy(10), radius_scipy),\n",
    "    (\"SciPy 20\", build_scipy(20), radius_scipy),\n",
    "    (\"SciPy 50\", build_scipy(50), radius_scipy),\n",
    "    (\"SciPy 100\", build_scipy(100), radius_scipy),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1843e2a-7e21-452b-8477-b5e03e46bff5",
   "metadata": {},
   "source": [
    "The above plots should clearly demonstrate that a finetuning of the cutoff parameter is necessary to achieve optimal performance. In `py4dgeo`, the key factors for the trade-off decisions would be:\n",
    "\n",
    "* Ratio core-points/points in epoch\n",
    "* Maximum radius for radius search (or rather # of expected points in radius)\n",
    "\n",
    "The fact that PCL does not expose the cutoff parameter to users is a clear disadvantage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ad297-cd62-4a2b-b59f-e36340952127",
   "metadata": {},
   "source": [
    "## Comparison on LAS data set\n",
    "\n",
    "The extensive above comparison with points distributed across the unit cube might introduce biases based on the structure of the test cloud. We will therefore verify our findings with a Lidar data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f89b87-86e8-439d-9963-71b3392220a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('/home/jovyan/shared/uls_thingstaette.xyz', delimiter=' ', dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3741204a-e526-4813-8d23-235eb2df2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_comparison(impls, radius=1.0):\n",
    "    def print_times(name, build_func, radius_func):\n",
    "        build_time, cloud = minimum_across_runs(10, build_func, data)\n",
    "        query_time, result = minimum_across_runs(10, radius_func, cloud, data[10000], radius)\n",
    "        print(f\"{name} - Build time: {build_time} - Query time {query_time}\")\n",
    "    for n, bf, rf in impls:\n",
    "        print_times(n, bf, rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dfa86d-a8bb-48b0-b81a-804de1d16e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_comparison([\n",
    "    (\"PCL KDTree\", build_pcl_kdtree, radius_mine),\n",
    "    (\"SciPy KDTree\", build_scipy(), radius_scipy),\n",
    "    (\"NanoFLANN\", build_nanoflann(), radius_mine),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c389a1-cda4-4a63-acb8-2fa21a2615d8",
   "metadata": {},
   "source": [
    "The results do not seem to vary too much from what we have seen before, but again we see that SciPy seems to work better for larger return sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f531eb1-b735-4a35-b267-258547d23384",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_comparison([\n",
    "    (\"PCL KDTree\", build_pcl_kdtree, radius_mine),\n",
    "    (\"SciPy KDTree\", build_scipy(), radius_scipy),\n",
    "    (\"NanoFLANN\", build_nanoflann(), radius_mine),\n",
    "], radius=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158a9ea-a694-4f8c-aee4-8448423a1db6",
   "metadata": {},
   "source": [
    "Finally, we double-check that our experiments are qualitatively reproduced for a very large dataset of ~250M points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0341040-3f6e-4806-ba0b-4e9ec0dfc086",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = laspy.read(\"/home/dominic/Downloads/ahk_2017_full.laz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ade502-3c98-42f1-a9ef-8618a197237e",
   "metadata": {},
   "source": [
    "## Decision\n",
    "\n",
    "I am leaning towards a NanoFLANN implementation. A summary of reasons:\n",
    "\n",
    "* Single Header with BSD License -> Copy into project, no installation etc.\n",
    "* Flexible handling of input: We can directly operate on `numpy.array` without ugly tricks\n",
    "* Very competitive performance in above comparisons\n",
    "* PDAL also uses it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
