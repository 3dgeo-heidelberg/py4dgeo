{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364f98be",
   "metadata": {},
   "source": [
    "# Outlier removal with SOR and ScOR using `py4dgeo`\n",
    "\n",
    "This notebook demonstrates how to perform automated outlier removal in terrestrial and permanent laser scanning (TLS/PLS) point clouds using two complementary approaches implemented in `py4dgeo`:\n",
    "\n",
    "1. **Statistical Outlier Removal (SOR)** – a widely used, distance-based filter that detects points whose local neighbor distances deviate strongly from the global distribution ([Rusu et al., 2008](#References)).\n",
    "2. **Scan Outlier Ratio (ScOR)** – a LiDAR scanning- and survey-aware descriptor that compares *expected* and *observed* point spacing in the scanners angular domain and is specifically designed for TLS/PLS data.\n",
    "\n",
    "Both methods are applied to a small multi-temporal example data set consisting of three epochs acquired from the same scan position:\n",
    "\n",
    "- `t1` – reference epoch (to be filtered)\n",
    "- `t2` – alternative epoch used as a multi-temporal neighborhood\n",
    "- `t3` – further epoch for temporally aggregated neighborhoods\n",
    "\n",
    "In line with the associated ScOR paper ([Tabernig and Höfle, 2026](#References)), the focus is on:\n",
    "\n",
    "- removing **detached and transient points** (e.g. insects, rain, ghost points, temporary objects),\n",
    "- **preserving coherent surfaces** that are suitable for 3D surface change analysis,\n",
    "- and demonstrating how **multi-temporal neighborhoods** can be used to detect and remove large transient objects.\n",
    "\n",
    "We first run SOR on the reference epoch, then compute ScOR for:\n",
    "- a **single epoch** (standard ScOR),\n",
    "- a **single other epoch** (bi-temporal ScOR),\n",
    "- and **temporally aggregated epochs** (multi-temporal ScOR).\n",
    "\n",
    "The results are stored in LAS files (via `py4dgeo.Vapc`) so they can later be explored and thresholded interactively, e.g. in CloudCompare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a05bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py4dgeo\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916987ca",
   "metadata": {},
   "source": [
    "## Statistical Outlier Removal (SOR) on the first epoch\n",
    "\n",
    "We start by importing the required packages and defining the input and output files:\n",
    "\n",
    "- `t1_file` – the first epoch, which we want to filter.\n",
    "- `t2_file`, `t3_file` – additional epochs used later as **multi-temporal neighborhood candidates**.\n",
    "- `SOR_out` – output LAS file where the SOR results for epoch 1 are stored.\n",
    "\n",
    "We also specify the additional dimensions `return_number` and `number_of_returns`, which will later allow us to restrict neighborhoods to **last returns** only (as recommended in the ScOR paper).\n",
    "\n",
    "Next, we configure and run the SOR filter:\n",
    "\n",
    "- `k` – number of nearest neighbors used to compute local mean distances,\n",
    "- `std_dev_multiplier` – global threshold on mean distances, expressed in units of standard deviation of the global distribution,\n",
    "- `remove_points=False` – **we do not remove points yet**, but instead keep an inlier/outlier flag for inspection and further processing.\n",
    "\n",
    "SOR is applied to `t1` and returns:\n",
    "\n",
    "- the (possibly updated) epoch `search_points_epoch`,\n",
    "- an `inlier_outlier` flag per point,\n",
    "- the mean neighbor distance for each point (`mean_distances`).\n",
    "\n",
    "We then wrap the result in a `py4dgeo.Vapc` object, store the per-point mean distances and the inlier/outlier flag as attributes, and write them to `SOR_out`. This creates a SOR-annotated point cloud that we will use as input for ScOR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c0567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_file = r\"E:\\test_data\\240102_070000 - SINGLESCANS - 240102_070700.laz\" # first epoch, the one to be filtered\n",
    "t2_file = r\"E:\\test_data\\240102_080000 - SINGLESCANS - 240102_080650.laz\" # second epoch, neighborhood candidates (for single other epoch multi-temporal)\n",
    "t3_file = r\"E:\\test_data\\240102_090000 - SINGLESCANS - 240102_090653.laz\" # third epoch, neighborhood candidates (for aggregated-epochs multi-temporal)\n",
    "\n",
    "SOR_out = t1_file.replace(\".laz\",\"_SOR_py4dgeo.laz\")\n",
    "dims = {\"return_number\": \"return_number\",\n",
    "        \"number_of_returns\": \"number_of_returns\"}\n",
    "\n",
    "k= 8\n",
    "std_dev_multiplier=1.0\n",
    "# Lets not remvoe the points directly but store the inlier/outlier flag instead.\n",
    "remove_points = False \n",
    "\n",
    "search_points_epoch = py4dgeo.read_from_las(t1_file, additional_dimensions=dims)\n",
    "SOR = py4dgeo.SOR(\n",
    "    epoch = search_points_epoch,\n",
    "    k = k,\n",
    "    std_dev_multiplier=std_dev_multiplier,\n",
    "    remove_points=remove_points,\n",
    ")\n",
    "\n",
    "search_points_epoch,inlier_outlier, mean_distances = SOR.run()\n",
    "\n",
    "vp_ob = py4dgeo.Vapc(search_points_epoch, voxel_size=0.01)\n",
    "vp_ob.out[\"mean_distance_%s\"%k] = mean_distances\n",
    "if remove_points is False:\n",
    "        vp_ob.out[\"inlier_outlier_%s\"%k] = inlier_outlier\n",
    "vp_ob.save_as_las(SOR_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100aec9",
   "metadata": {},
   "source": [
    "## Restricting neighborhoods to last returns\n",
    "\n",
    "ScOR, as defined in the paper, is primarily evaluated on **last and single returns**. Intermediate returns within vegetation or complex structures often represent semi-transparent or transient objects (e.g. canopy layers, moving leaves) that violate local surface assumptions.\n",
    "\n",
    "To enforce this, we create a small helper function `mask_last_returns`:\n",
    "\n",
    "- It takes an `Epoch` and the names of the fields `return_number` and `number_of_returns`.\n",
    "- It builds a boolean mask where `return_number == number_of_returns`, i.e. **last returns only**.\n",
    "- It returns a new `Epoch` containing only those last-return points and their additional dimensions.\n",
    "\n",
    "This function is used later to build consistent, surface-focused neighborhoods for ScOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb269084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_last_returns(neighborhood_candidates1: py4dgeo.Epoch, return_number_field_name: str, number_of_returns_field_name: str) -> py4dgeo.Epoch:\n",
    "    \"\"\"Return a new Epoch containing only the last returns from the input Epoch.\"\"\"\n",
    "    ad = neighborhood_candidates1.additional_dimensions\n",
    "    rn = ad[return_number_field_name].astype(np.int32).ravel()\n",
    "    nr = ad[number_of_returns_field_name].astype(np.int32).ravel()\n",
    "    mask = (rn == nr)\n",
    "    last_return_epoch = py4dgeo.Epoch(\n",
    "        cloud=neighborhood_candidates1.cloud[mask],\n",
    "        additional_dimensions=ad[mask],\n",
    "    )\n",
    "    return last_return_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60974a1c",
   "metadata": {},
   "source": [
    "## Single-epoch ScOR\n",
    "\n",
    "In the next step, we compute **ScOR** for the first epoch using only its own points as neighborhood candidates:\n",
    "\n",
    "1. We define additional dimensions to be loaded:\n",
    "   - `return_number` and `number_of_returns` (for last-return masking),\n",
    "   - `inlier_outlier_k` (here with `k = 8`) to keep track of the SOR result.\n",
    "\n",
    "2. We specify:\n",
    "   - `scan_position` – scanner location in 3D (here `[0, 0, 0]`),\n",
    "   - `scan_resolution` – nominal angular step in radians (here an adjusted value),\n",
    "   - `increment` – step size in the scan grid used to define neighbors (here `0.5`).\n",
    "\n",
    "   To reduce aliasing effects (as described in the paper), we slightly **adjust** the scan resolution by multiplying with 2 and adding a small offset. This grid artefacts when discretizing the scan angles for neighborhood construction.\n",
    "\n",
    "3. We read `search_points_t1` from `t1_file` (the SOR output) and use `mask_last_returns` to create `neighborhood_candidates_t1` containing **only last returns**.\n",
    "\n",
    "4. We instantiate `py4dgeo.ScOR` with:\n",
    "   - the full point cloud as `search_point_epoch`,\n",
    "   - last returns from the same epoch as `neighborhood_candidate_epochs`,\n",
    "   - scan position and scan resolution,\n",
    "   - increment to define the neighborhood in the angular grid.\n",
    "\n",
    "When we call `ScOR.run()`, we obtain:\n",
    "\n",
    "- `scor_value_standard` – the ScOR value for each point in `search_points_t1`,  \n",
    "  which gives a value between 0.0 (detached/outlier-like) and 1.0 (fully consistent surface).\n",
    "- `expected_distance_standard` – the expected neighbor distance in 3D, derived from scan geometry.\n",
    "- `observed_distance_standard` – the actually measured 3D neighbor distance in object space.\n",
    "\n",
    "These quantities directly reflect the **local surface consistency** in the scan geometry, which is the main idea behind ScOR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e69b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_file = SOR_out\n",
    "\n",
    "dims_search_points = {\"return_number\": \"return_number\",\n",
    "        \"number_of_returns\": \"number_of_returns\",\n",
    "        \"inlier_outlier_%s\"%k:\"inlier_outlier_%s\"%k} # we keep the SOR result as well\n",
    "                                                      \n",
    "return_number_field = \"return_number\"\n",
    "number_of_returns_field = \"number_of_returns\"\n",
    "dims_neighborhood_candidates = {return_number_field: return_number_field,\n",
    "        number_of_returns_field: number_of_returns_field}\n",
    "\n",
    "\n",
    "scan_position = [0,0,0]\n",
    "scan_resolution = 0.015 # regular scan resolution\n",
    "# To avoid aliasing effects, we adjust the scan resolution slightly (2x + small offset)\n",
    "scan_resolution = scan_resolution*2 + 0.0028 # adjusted to prevent aliasing effects\n",
    "increment = 0.5\n",
    "\n",
    "# Read the point cloud\n",
    "search_points_t1 = py4dgeo.read_from_las(t1_file, additional_dimensions=dims_search_points)\n",
    "neighborhood_candidates_t1 = mask_last_returns(search_points_t1, \"return_number\", \"number_of_returns\")\n",
    "\n",
    "ScOR = py4dgeo.ScOR(\n",
    "    search_point_epoch=search_points_t1,\n",
    "    neighborhood_candidate_epochs=neighborhood_candidates_t1,\n",
    "    scan_position=scan_position,\n",
    "    scan_resolution=scan_resolution,\n",
    "    increment=increment\n",
    ")\n",
    "\n",
    "scor_value_standard, expected_distance_standard, observed_distance_standard = ScOR.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5d700",
   "metadata": {},
   "source": [
    "## Multi-temporal ScOR: single other epoch as neighborhood\n",
    "\n",
    "So far, neighborhoods were defined **within the same epoch**. For permanent laser scanning (PLS) setups, we can also exploit **multi-temporal neighborhoods**, i.e. build neighborhoods from different epochs acquired from the same scan position.\n",
    "\n",
    "In this section we:\n",
    "\n",
    "- Fix epoch 1 (`search_points_t1`) as the **reference epoch**, and\n",
    "- Use epoch 2 (`t2_file`) as the **only neighborhood candidate**.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Set `scan_position` and `scan_resolution` for this multi-temporal experiment.\n",
    "2. Read `neighborhood_candidates_t2` from `t2_file` and again restrict to **last returns** using `mask_last_returns`.\n",
    "3. Instantiate a new `py4dgeo.ScOR` object with:\n",
    "   - `search_point_epoch = search_points_t1` (epoch 1),\n",
    "   - `neighborhood_candidate_epochs = neighborhood_candidates_t2` (epoch 2),\n",
    "\n",
    "`ScOR.run()` then computes:\n",
    "\n",
    "- `scor_value_to_t2` – ScOR values for points of epoch 1, but **using neighborhoods from epoch 2**.\n",
    "- `expected_distance_to_t2` and `observed_distance_to_t2` – analogous to the single-epoch case, but now based on cross-epoch neighborhoods.\n",
    "\n",
    "Conceptually, this configuration highlights **temporal inconsistency**:  \n",
    "points that represent transient objects (e.g. a person present only in one epoch) will obtain **low ScOR values**, because their local neighborhood in the other epoch is dominated by different structures (typically the background surface).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf3f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_candidates_t2 = py4dgeo.read_from_las(t2_file,additional_dimensions=dims_neighborhood_candidates)\n",
    "neighborhood_candidates_t2 = mask_last_returns(neighborhood_candidates_t2, \"return_number\", \"number_of_returns\")\n",
    "\n",
    "scor = py4dgeo.ScOR(\n",
    "    search_point_epoch=search_points_t1,\n",
    "    neighborhood_candidate_epochs=neighborhood_candidates_t2,\n",
    "    scan_position=scan_position,\n",
    "    scan_resolution=scan_resolution,\n",
    "    increment=increment\n",
    ")\n",
    "scor_value_to_t2, expected_distance_to_t2, observed_distance_to_t2 = scor.run()\n",
    "print(\"Finito single-other-epoch neighborhood ScOR run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c6b447",
   "metadata": {},
   "source": [
    "## Multi-temporal ScOR: temporally aggregated neighborhoods\n",
    "\n",
    "In many PLS applications, we are interested not only in pairwise comparisons, but also in **temporally aggregated neighborhoods** ([Tabernig et al., 2025](#References)). Instead of a single other epoch, we aggregate several epochs into one neighborhood candidate set.\n",
    "\n",
    "Here we:\n",
    "\n",
    "1. Read `neighborhood_candidates_t3` from `t3_file`.\n",
    "2. Ensure that both `neighborhood_candidates_t2` and `neighborhood_candidates_t3` are again restricted to **last returns**.\n",
    "3. Build an aggregated, multi-epoch neighborhood consisting of:\n",
    "   - `neighborhood_candidates_t1` (epoch 1),\n",
    "   - `neighborhood_candidates_t2` (epoch 2),\n",
    "   - `neighborhood_candidates_t3` (epoch 3).\n",
    "\n",
    "These are passed as a **tuple of epochs** to `py4dgeo.ScOR` as `neighborhood_candidate_epochs`.\n",
    "\n",
    "When calling `ScOR.run()` in this configuration, we obtain:\n",
    "\n",
    "- `scor_value_aggregated` – ScOR values for epoch 1 points, but now using a **multi-epoch aggregated neighborhood**.\n",
    "- `expected_distance_aggregated` and `observed_distance_aggregated` – expected and observed neighbor distances in this aggregated neighborhood.\n",
    "\n",
    "This setup corresponds to the **Level of Aggregation (LoA)** described in the paper:\n",
    "\n",
    "- It tends to **stabilize neighborhoods** by averaging over several epochs.\n",
    "- Large transient objects (e.g. a person present in only one epoch) become very inconsistent relative to the aggregated background and thus obtain very low ScOR values.\n",
    "- Persistent surfaces (rock outcrop, ground, walls, etc.) keep similar neighborhoods across epochs and remain with high ScOR values.\n",
    "\n",
    "Aggregated neighborhoods therefore help to **robustly detect dynamic objects** in PLS time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952049b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_candidates_t3 = py4dgeo.read_from_las(t3_file, additional_dimensions=dims_neighborhood_candidates)\n",
    "neighborhood_candidates_t3 = mask_last_returns(neighborhood_candidates_t3, \"return_number\", \"number_of_returns\")\n",
    "\n",
    "scor = py4dgeo.ScOR(\n",
    "    search_point_epoch=search_points_t1,\n",
    "    neighborhood_candidate_epochs=(neighborhood_candidates_t1, neighborhood_candidates_t2, neighborhood_candidates_t3),\n",
    "    scan_position=scan_position,\n",
    "    scan_resolution=scan_resolution,\n",
    "    increment=increment)\n",
    "\n",
    "scor_value_aggregated, expected_distance_aggregated, observed_distance_aggregated = scor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd174c9",
   "metadata": {},
   "source": [
    "## Exporting ScOR diagnostics to LAS for further analysis\n",
    "\n",
    "Finally, we collect all ScOR-related quantities into a single `py4dgeo.Vapc` object for the reference epoch:\n",
    "\n",
    "- From the **single-epoch ScOR** run:\n",
    "  - `ScOR_standard`\n",
    "  - `expected_distance_standard`\n",
    "  - `observed_distance_standard`\n",
    "\n",
    "- From the **bi-temporal ScOR** run (epoch 1 vs epoch 2):\n",
    "  - `ScOR_to_t2`\n",
    "  - `expected_distance_to_t2`\n",
    "  - `observed_distance_to_t2`\n",
    "\n",
    "- From the **aggregated multi-temporal ScOR** run (epochs 1–3):\n",
    "  - `ScOR_aggregated`\n",
    "  - `expected_distance_aggregated`\n",
    "  - `observed_distance_aggregated`\n",
    "\n",
    "These arrays are stored as additional per-point attributes in a `Vapc` object and written to the output file `ScOR_out`.\n",
    "\n",
    "This LAS file can now be:\n",
    "\n",
    "- loaded into **CloudCompare** (or other point cloud tools),\n",
    "- visualized with ScOR fields as color scales,\n",
    "- and thresholded (e.g. using a value around `ScOR ≤ 0.11` as discussed in the paper) to:\n",
    "\n",
    "  - remove detached points, small transient clusters (e.g. insects, leaves),\n",
    "  - and detect large temporary objects via multi-temporal neighborhoods.\n",
    "\n",
    "Together with the SOR flag and distances, this provides a **complete outlier-removal toolkit** that combines:\n",
    "\n",
    "- SOR’s global distance-based filtering, and  \n",
    "- ScOR’s scan-aware, range-robust measure of local surface coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a18bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScOR_out = t1_file.replace(\".laz\",\"_py4dgeo.laz\")\n",
    "\n",
    "vapc = py4dgeo.Vapc(search_points_t1,voxel_size=0.01)\n",
    "vapc.out[\"ScOR_standard\"] = scor_value_standard\n",
    "vapc.out[\"expected_distance_standard\"] = expected_distance_standard\n",
    "vapc.out[\"observed_distance_standard\"] = observed_distance_standard\n",
    "\n",
    "vapc.out[\"ScOR_to_t2\"] = scor_value_to_t2\n",
    "vapc.out[\"expected_distance_to_t2\"] = expected_distance_to_t2\n",
    "vapc.out[\"observed_distance_to_t2\"] = observed_distance_to_t2\n",
    "\n",
    "vapc.out[\"ScOR_aggregated\"] = scor_value_aggregated\n",
    "vapc.out[\"expected_distance_aggregated\"] = expected_distance_aggregated \n",
    "vapc.out[\"observed_distance_aggregated\"] = observed_distance_aggregated\n",
    "\n",
    "vapc.save_as_las(ScOR_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e90df3",
   "metadata": {},
   "source": [
    "### References:\n",
    "* Rusu, R.B., Marton, Z.C., Blodow, N., Dolha, M., Beetz, M., 2008. Towards 3D Point cloud based object maps for household environments. Robot. Auton. Syst. 56, 927–941. doi.org/10.1016/j.robot.2008.08.005\n",
    "* Tabernig, R., Albert, W., Weiser, H., Fritzmann, P., Anders, K., Rutzinger, M., Höfle, B., 2025. Temporal aggregation of point clouds improves permanent laser scanning of landslides in forested areas. Sci. Remote Sens. 12, 100254. doi.org/10.1016/j.srs.2025.100254\n",
    "* Tabernig, R., Höfle, B., 2026. Scan Outlier Ratio (ScOR): LiDAR Scanning and Survey-Aware Filtering of Detached Points in Terrestrial and Permanent Laser Scanning Point Clouds\n",
    "(forthcoming)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py4dgeo_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
