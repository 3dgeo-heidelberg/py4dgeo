{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Applying PB-M3C2 in long term monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In applications where data from the same observation site is acquired over a long period of time, it is desirable to carry out the training of the PB-M3C2 algorithm once and then apply the trained model to newly acquired epochs. This notebook explains how this process is implemented in `py4dgeo`. First, we are carrying out the training procedure like we did in the explanation of the [base workflow](pbm3c2.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py4dgeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "py4dgeo.set_interactive_backend(\"vtk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch0, epoch1 = py4dgeo.read_from_xyz(\n",
    "    \"plane_horizontal_t1.xyz\", \"plane_horizontal_t2.xyz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = py4dgeo.PBM3C2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz_epoch0, xyz_epoch1, segment_id = alg.export_segmented_point_cloud_and_segments(\n",
    "    epoch0=epoch0,\n",
    "    epoch1=epoch1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg.training(\n",
    "    extracted_segments_file_name=\"extracted_segments.seg\",\n",
    "    extended_y_file_name=\"testdata-labelling.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Having the pre-trained algorithm object `alg`, we would like to save it for reuse in later analysis sessions. We do use Python's `pickle` module for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"alg.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(alg, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Then, in a subsequent session, we can reload the algorithm using `pickle`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"alg.pickle\", \"rb\") as infile:\n",
    "    alg = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "We can then feed new epochs (here, we just use `epoch0` again) into the algorithm. It will apply segmentation on the new epoch and then run the prediction for the new epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    _0,\n",
    "    _1,\n",
    "    extracted_segments_epoch0,\n",
    ") = alg.export_segmented_point_cloud_and_segments(\n",
    "    # must be a new epoch\n",
    "    epoch0=epoch0,\n",
    "    # epoch1=None,\n",
    "    x_y_z_id_epoch0_file_name=None,\n",
    "    x_y_z_id_epoch1_file_name=None,\n",
    "    extracted_segments_file_name=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "We can then calculate distances for the new epoch. Note, that in order to disable those parts of the analysis pipeline that are already computed for the reference epoch, we pass the constant dictionary `**py4dgeo.config_epoch0_as_segments`. If you have customized the analysis pipeline, you should adapt the configuration settings accordingly and disable all those steps that are not required for the reference epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, uncertainties = alg.compute_distances(\n",
    "    epoch0=extracted_segments_epoch0, epoch1=epoch1, **py4dgeo.config_epoch0_as_segments\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
