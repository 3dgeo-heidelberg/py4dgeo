{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e82ada",
   "metadata": {},
   "source": [
    "## 3D Change Analysis at an Active Rock Glacier using Multitemporal Point Clouds\n",
    "\n",
    "In this notebook, we will perform a surface change analysis on TLS point clouds of the Äußeres Hochebenkar rock glacier (46°50’11''N, 11°00’20‘’E) for two consecutive years. An introduction to the case study and dataset can be found in the e-learning course ETRAINEE [here](https://3dgeo-heidelberg.github.io/etrainee/data_usecases/usecase_rockglacier_ahk.html). \n",
    "\n",
    "[E-TRAINEE](https://github.com/3dgeo-heidelberg/etrainee) is an e-learning course on Time Series Analysis in Remote Sensing for Understanding Human-Environment Interactions. This course has been developed by research groups from four partner universities – Charles University, Heidelberg University, University of Innsbruck, and University of Warsaw. \n",
    "\n",
    "The objective is to perform a full workflow of 3D change analysis with\n",
    "\n",
    "* Assessment of alignment uncertainty\n",
    "* Change analysis using the M3C2 algorithm \n",
    "* Change representation and assessment of results\n",
    "* For the fast ones: Comparison to change analysis via DEM differencing\n",
    "\n",
    "Look into the article by [Zahs et al., 2019](https://doi.org/10.1002/ppp.2004) for comparison of possible surface dynamics at the site and help for deciding on suitable parameters, etc.\n",
    "\n",
    "The workflow is  introduced throughout this notebook. Also, make use of the [software documentation](https://py4dgeo.readthedocs.io/en/latest/pythonapi.html)!\n",
    "## Software and data\n",
    "This task is solved using Python with the [`py4dgeo`](https://github.com/3dgeo-heidelberg/py4dgeo) library. \n",
    "\n",
    "You can use CloudCompare or GIS Software (e.g. QGIS) to check the data and visualize your results.\n",
    "\n",
    "The dataset will be two epochs of point clouds acquired by UAV laser scanning in 2020 and 2021: `ahk/ahk_2020_uls.laz` and `ahk/ahk_2021_uls.laz`. Both point clouds [can be downloaded](https://zenodo.org/records/10003575) (module3.zip) from the E-learning course E-TRAINEE in the data directory `ahk`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf01a9b",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "First, we start by setting up the Python environment and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af980c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import py4dgeo\n",
    "import os\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pooch\n",
    "\n",
    "# Download data from zenodo and set path to point cloud folder\n",
    "p = pooch.Pooch(base_url=\"doi:10.5281/zenodo.10003574/\", path=pooch.os_cache(\"py4dgeo\"))\n",
    "p.load_registry_from_doi()\n",
    "p.fetch(\"module3.zip\", processor=pooch.Unzip())\n",
    "pc_dir = p.path / \"module3.zip.unzip\" / \"module3\" / \"ahk\"\n",
    "\n",
    "# list of point clouds (time series)\n",
    "pc_list = os.listdir(pc_dir)\n",
    "pc_list[:5]  # print the first elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf87688",
   "metadata": {},
   "source": [
    "Specify the path to the data and names of input files, and read in the point cloud data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c69b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive the file paths and read in the data\n",
    "pc_file_2020 = pc_dir / \"ahk_2020_uls.laz\"\n",
    "pc_file_2021 = pc_dir / \"ahk_2021_uls.laz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bf3d05-b733-4e80-a960-1a4189d5e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_2020 = py4dgeo.read_from_las(pc_file_2020).cloud\n",
    "pc_2021 = py4dgeo.read_from_las(pc_file_2021).cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a56f2c-b527-433a-8c1e-9607c1c88bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a1295",
   "metadata": {},
   "source": [
    "## Assessment of alignment uncertainty\n",
    "The epochs are georeferenced, i.e., multitemporal point clouds are registered in a common coordinate reference frame. The point clouds have further been fine aligned using an ICP method on stable parts outside the rock glacier area. You may assume that the best possible alignment between the multitemporal data has been achieved.\n",
    "\n",
    "For change analysis, it is important to assess the uncertainty of the point cloud alignment for information on the minimum detectable change. Do this by cutting out some stable rock surfaces outside the rock glacier and checking the cloud-to-cloud distances for these subsets. You may cut out point cloud subsets manually in CloudCompare, or define polygons (e.g., in QGIS) to extract the parts from the full point cloud in the Python workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d9db8",
   "metadata": {},
   "source": [
    "We check the alignment uncertainty by cutting out some stable rock surfaces outside the rock glacier. In this solution, we have derived polygons of stable parts for one point cloud epoch (`ahk_2021_uls_stableparts.shp`). Using this, we can extract the points within stable parts (2D query) and subsequently derive the point cloud distances between epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify input shapefile\n",
    "shp_file = pc_dir / \"ahk_2021_uls_stableparts.shp\"\n",
    "\n",
    "# read the shapefile\n",
    "datasource = gpd.read_file(shp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a6774",
   "metadata": {},
   "source": [
    "In the next part, we check each remaining point for its position in the polygon/stable area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e58d1e8-98b8-4d60-87a8-3bc9e0b142d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mask stable parts via polygons for 2020 points...\")\n",
    "pc_2020_gdf = gpd.GeoDataFrame(\n",
    "    geometry=gpd.points_from_xy(pc_2020[:, 0], pc_2020[:, 1], pc_2020[:, 2]),\n",
    "    crs=\"EPSG:25832\",\n",
    ")\n",
    "pts_in_poly_2020_gdf = gpd.sjoin(pc_2020_gdf, datasource, predicate=\"within\")\n",
    "pc_stable_2020 = pc_2020[pts_in_poly_2020_gdf.index]\n",
    "\n",
    "print(\"Mask stable parts via polygons for 2021 points...\")\n",
    "pc_2021_gdf = gpd.GeoDataFrame(\n",
    "    geometry=gpd.points_from_xy(pc_2021[:, 0], pc_2021[:, 1], pc_2021[:, 2]),\n",
    "    crs=\"EPSG:25832\",\n",
    ")\n",
    "pts_in_poly_2021_gdf = gpd.sjoin(pc_2021_gdf, datasource, predicate=\"within\")\n",
    "pc_stable_2021 = pc_2021[pts_in_poly_2021_gdf.index]\n",
    "print(\n",
    "    f\"Remaining points: {pc_stable_2020.shape[0]}/{pc_2020.shape[0]} for 2020; {pc_stable_2021.shape[0]}/{pc_2021.shape[0]} for 2021\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582019f6",
   "metadata": {},
   "source": [
    "Create a kd tree and compute the distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3652df08-b1d9-4953-ae76-873c0e8f1323",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_stable_2020 = py4dgeo.Epoch(pc_stable_2020)\n",
    "epoch_stable_2021 = py4dgeo.Epoch(pc_stable_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b900ff-c935-4682-8d97-2e4587c08fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_stable_2021.build_kdtree()\n",
    "indices, distances = epoch_stable_2021.kdtree.nearest_neighbors(epoch_stable_2020.cloud)\n",
    "distances = np.sqrt(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1970def",
   "metadata": {},
   "source": [
    "We assess the distances visually, and derive the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7920ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow interactive rotation in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# create a figure with 3D axis\n",
    "fig, ax = plt.subplots(1, 1, subplot_kw={\"projection\": \"3d\"}, figsize=(7, 5))\n",
    "\n",
    "nth = 100\n",
    "\n",
    "# plot the point cloud colored by height (z values)\n",
    "s = ax.scatter(\n",
    "    pc_stable_2020[::nth, 0],\n",
    "    pc_stable_2020[::nth, 1],\n",
    "    pc_stable_2020[::nth, 2],\n",
    "    s=1,\n",
    "    c=distances[::nth],\n",
    ")\n",
    "\n",
    "# label axes and add title\n",
    "ax.set_xlabel(\"X [m]\")\n",
    "ax.set_ylabel(\"Y [m]\")\n",
    "ax.set_zlabel(\"Z [m]\")\n",
    "\n",
    "# set initial view of 3D plot\n",
    "ax.view_init(elev=40.0, azim=130.0)\n",
    "\n",
    "# add a colorbar\n",
    "fig.colorbar(s, shrink=0.5, aspect=10, label=\"NN distance [m]\", ax=ax, pad=0.2)\n",
    "\n",
    "# show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3629de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print statistics of nn distances\n",
    "print(f\"Median distances: {np.median(distances):.3f} m\")\n",
    "print(f\"Std. dev. of distances: {np.std(distances):.3f} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166b913",
   "metadata": {},
   "source": [
    "The distances are quite large according to the statistics. We would assume lower values for the (visual) quality of the co-registration and the measurement accuracy of UAV laser scanning in this scene. An explanation for the large values could be the influence of point sampling, so you see here an effect of the drawbacks in simple cloud-to-cloud (C2C) distance computation. For a potentially more robust result, we derive the M3C2 distances in these stable parts (for a detailed solution how to establish the M3C2 change analysis, see the remainder of the solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cdf447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the M3C2 algorithm object, using all points of 2020 as corepoints (no subsampling)\n",
    "m3c2 = py4dgeo.M3C2(\n",
    "    epochs=(epoch_stable_2020, epoch_stable_2021),\n",
    "    corepoints=epoch_stable_2020.cloud[::],\n",
    "    normal_radii=(0.5,),\n",
    "    cyl_radii=(0.5,),\n",
    "    max_distance=(5.0),\n",
    "    registration_error=(0.0),\n",
    ")\n",
    "\n",
    "# run the distance computation\n",
    "m3c2_distances_stableparts, uncertainties_stableparts = m3c2.run()\n",
    "\n",
    "# print statistics of nn distances\n",
    "print(f\"Median M3C2 distances: {np.nanmedian(m3c2_distances_stableparts):.3f} m\")\n",
    "print(f\"Std. dev. of M3C2 distances: {np.nanstd(m3c2_distances_stableparts):.3f} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dbfd91",
   "metadata": {},
   "source": [
    "As the M3C2 distances provide a more robust etimate of point distances in the stable parts, we use their standard deviation as measure of alignment accuracy in our change analysis. This measure we can later on use as registration error for deriving the level of detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06bab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_2020_2021 = np.nanstd(m3c2_distances_stableparts)\n",
    "print(\n",
    "    f\"Registration error based on point distances in stable parts is {reg_2020_2021:.3f} m.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68894990",
   "metadata": {},
   "source": [
    "This value already provides as an estimate of change magnitudes which can be confidently derived from point cloud comparison of these two epochs. In the M3C2 distance calculation, we will derive the per-point level of detection considering the local (i.e. spatially variable) roughness and point density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3400fba8",
   "metadata": {},
   "source": [
    "## 3D change analysis via point cloud distance computation\n",
    "\n",
    "Calculate the distance between point clouds of the two epochs using the M3C2 algorithm ([Lague et al., 2013](https://doi.org/10.1016/j.isprsjprs.2013.04.009  )). \n",
    "\n",
    "Think about a suitable parametrization:\n",
    "* Normal scale D: diameter of point neighborhood to use for normal vector computation. *Hint*: Aim for overall, larger-scale surface change, e.g. due to rock glacier creep or heave/thaw (instead of individual boulders).\n",
    "* Projection scale d: diameter of cylindrical point neighborhood to obtain average position.\n",
    "* Maximum cylinder depth: Maximum distance along normal at which to obtain distances to the other point cloud.\n",
    "* Preferred normal orientation.\n",
    "* Registration error ~ alignment accuracy\n",
    "\n",
    "Consider using the [py4dgeo documentation](https://py4dgeo.readthedocs.io/en/latest/intro.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e000f438",
   "metadata": {},
   "source": [
    "To perform the M3C2 change analyis with py4dgeo, we need to read the data into `Epoch` objects. This can be done from the `Nx3` array of coordinates using the function `py4dgeo.epoch.as_epoch()`, or directly from file using `py4dgeo.read_from_las()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec899db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load epochs into py4dgeo objects\n",
    "epoch2020 = py4dgeo.Epoch(pc_2020)\n",
    "epoch2021 = py4dgeo.Epoch(pc_2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ce160",
   "metadata": {},
   "source": [
    "As core points, we use a subsampling of coordinates in the epoch of 2020 (which will be the reference for deriving M3C2 distances to the later epoch of 2021), e.g. using every 100th point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b6f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use every nth point as core point for distance calculation\n",
    "corepoints = epoch2020.cloud[::100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d8ceaa",
   "metadata": {},
   "source": [
    "We parametrize the M3C2 algorithm object according to the parameters in [Zahs et al., 2019](#references), and use the registration error we derived above. The default normal orientation in `py4dgeo` is upwards (`up`), so we do not need to adapt/set this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256a187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate and parametrize the M3C2 algorithm object\n",
    "m3c2 = py4dgeo.M3C2(\n",
    "    epochs=(epoch2020, epoch2021),\n",
    "    corepoints=corepoints,\n",
    "    normal_radii=(4.0, 0.5, 6.0),\n",
    "    cyl_radii=(0.5,),\n",
    "    max_distance=(15.0),\n",
    "    registration_error=(reg_2020_2021),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea461f",
   "metadata": {},
   "source": [
    "Now the analysis can be run - you may have a coffee break in the meantime ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34415dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3c2_distances, uncertainties = m3c2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b401e6c1",
   "metadata": {},
   "source": [
    "All change information (distances and uncertainties) are now contained in the returned objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8dc256",
   "metadata": {},
   "source": [
    "## Change representation and assessment of results\n",
    "\n",
    "Visualize the obtained point cloud distances and corresponding information layers, such as the level of detection and the normal vectors representing the direction of changes.\n",
    "\n",
    "Prepare the result assessment for interpretation by analysts. For example, you may rasterize the different layers and create a map, e.g., of the derived surface changes in GIS software. Tip: Use the Web Map Service (WMS) of Tyrol as a basemap: [https://gis.tirol.gv.at/arcgis/services/Service_Public/orthofoto/MapServer/WMSServer](https://gis.tirol.gv.at/arcgis/services/Service_Public/orthofoto/MapServer/WMSServer).\n",
    "\n",
    "\n",
    "What are the different properties of visible changes and to which types of surface activity would you attribute them? Think of heave and subsidence processes, individual boulder movement, and rock glacier creep (cf. [Zahs et al., 2019](#references))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366b6129",
   "metadata": {},
   "source": [
    "Using the level of detection computed by the M3C2, we derive for each point if the quantified change is significant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a binary mask of significant change\n",
    "change_sign = np.where(abs(m3c2_distances) > uncertainties[\"lodetection\"], True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4951ff",
   "metadata": {},
   "source": [
    "Now we create a composite figure of the change values (distances), change directions (vertical component), level of detection and significant change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59792d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the figure\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 14), subplot_kw={\"projection\": \"3d\"})\n",
    "(ax1, ax2), (ax3, ax4) = axs\n",
    "\n",
    "# plot the distances\n",
    "d = ax1.scatter(\n",
    "    corepoints[:, 0],\n",
    "    corepoints[:, 1],\n",
    "    corepoints[:, 2],\n",
    "    c=m3c2_distances,\n",
    "    cmap=\"seismic_r\",\n",
    "    vmin=-5.0,\n",
    "    vmax=5.0,\n",
    "    s=1,\n",
    ")\n",
    "plt.colorbar(d, format=(\"%.2f\"), label=\"Distance [m]\", ax=ax1, shrink=0.5, pad=0.15)\n",
    "\n",
    "# plot the directions\n",
    "directions = m3c2.directions()\n",
    "dz = ax2.scatter(\n",
    "    corepoints[:, 0],\n",
    "    corepoints[:, 1],\n",
    "    corepoints[:, 2],\n",
    "    c=directions[:, 2],\n",
    "    cmap=\"cool\",\n",
    "    s=1,\n",
    ")\n",
    "plt.colorbar(dz, format=(\"%.2f\"), label=\"Normal Z [0,1]\", ax=ax2, shrink=0.5, pad=0.15)\n",
    "\n",
    "# plot the level of detection values\n",
    "l = ax3.scatter(\n",
    "    corepoints[:, 0],\n",
    "    corepoints[:, 1],\n",
    "    corepoints[:, 2],\n",
    "    c=uncertainties[\"lodetection\"],\n",
    "    cmap=\"viridis\",\n",
    "    vmin=0.0,\n",
    "    vmax=0.15,\n",
    "    s=1,\n",
    ")\n",
    "plt.colorbar(\n",
    "    l,\n",
    "    format=(\"%.2f\"),\n",
    "    label=\"Level of detection [m]\",\n",
    "    ax=ax3,\n",
    "    extend=\"max\",\n",
    "    shrink=0.5,\n",
    "    pad=0.15,\n",
    ")\n",
    "\n",
    "# plot the significant change values (boolean)\n",
    "# ax4.scatter(corepoints[~change_sign][:,0], corepoints[~change_sign][:,1], corepoints[~change_sign][:,2], label='Not significant change', c='blue', s=1) # if added, visibility of significant change areas is poor\n",
    "ax4.scatter(\n",
    "    corepoints[change_sign][:, 0],\n",
    "    corepoints[change_sign][:, 1],\n",
    "    corepoints[change_sign][:, 2],\n",
    "    label=\"Significant change\",\n",
    "    c=\"red\",\n",
    "    s=1,\n",
    ")\n",
    "ax4.legend()\n",
    "\n",
    "# add plot elements\n",
    "for ax_set in axs:\n",
    "    for ax in ax_set:\n",
    "        ax.set_xlabel(\"Easting [m]\")\n",
    "        ax.set_ylabel(\"Northing [m]\")\n",
    "        ax.set_aspect(\"equal\")\n",
    "        ax.view_init(elev=30.0, azim=120.0)\n",
    "\n",
    "plt.axis(\"equal\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc2b0b8",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Lague, D., Brodu, N., & Leroux, J. (2013). Accurate 3D comparison of complex topography with terrestrial laser scanner: Application to the Rangitikei canyon (N-Z). ISPRS Journal of Photogrammetry and Remote Sensing, 82, pp. 10-26. doi: [10.1016/j.isprsjprs.2013.04.009  ](https://doi.org/10.1016/j.isprsjprs.2013.04.009  ).\n",
    "\n",
    "* Zahs, V., Hämmerle, M., Anders, K., Hecht, S., Sailer, R., Rutzinger, M., Williams, J. G., & Höfle, B. (2019). Multi-temporal 3D point cloud-based quantification and analysis of geomorphological activity at an alpine rock glacier using airborne and terrestrial LiDAR. Permafrost and Periglacial Processes, 30 (3), pp. 222-238. doi: [10.1002/ppp.2004  ](https://doi.org/10.1002/ppp.2004  )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e66337-099b-4223-8554-03cd44ff8795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "bca379f2a991149e96af4e280266aaefc3952de62153de79b5ea1b1512730db6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
