{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e82ada",
   "metadata": {},
   "source": [
    "## 3D Change Analysis at an Active Rock Glacier using Multitemporal Point Clouds\n",
    "\n",
    "In this notebook, we will perform a surface change analysis on TLS point clouds of the Äußeres Hochebenkar rock glacier (46°50’11''N, 11°00’20‘’E) for two consecutive years. An introduction to the case study and dataset can be found in the e-learning course ETRAINEE [here](https://3dgeo-heidelberg.github.io/etrainee/data_usecases/usecase_rockglacier_ahk.html). \n",
    "\n",
    "[E-TRAINEE](https://github.com/3dgeo-heidelberg/etrainee) is an e-learning course on Time Series Analysis in Remote Sensing for Understanding Human-Environment Interactions. This course has been developed by research groups from four partner universities – Charles University, Heidelberg University, University of Innsbruck, and University of Warsaw. \n",
    "\n",
    "The objective is to perform a full workflow of 3D change analysis with\n",
    "\n",
    "* Assessment of alignment uncertainty\n",
    "* Change analysis using the M3C2 algorithm \n",
    "* Change representation and assessment of results\n",
    "* For the fast ones: Comparison to change analysis via DEM differencing\n",
    "\n",
    "Look into the article by [Zahs et al., 2019](https://doi.org/10.1002/ppp.2004) for comparison of possible surface dynamics at the site and help for deciding on suitable parameters, etc.\n",
    "\n",
    "The workflow is  introduced throughout this notebook. Also, make use of the software documentations!\n",
    "\n",
    "## Software and data\n",
    "This task is solved using Python with the [`py4dgeo`](https://github.com/3dgeo-heidelberg/py4dgeo) library. \n",
    "\n",
    "You can use CloudCompare or GIS Software (e.g. QGIS) to check the data and visualize your results.\n",
    "\n",
    "The dataset will be two epochs of point clouds acquired by UAV laser scanning in 2020 and 2021: `ahk/ahk_2020_uls.laz` and `ahk/ahk_2021_uls.laz`. Both point clouds [can be downloaded](https://zenodo.org/records/10003575) (module3.zip) from the E-learning course E-TRAINEE in the data directory `ahk`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf01a9b",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "First, we start by setting up the Python environment and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af980c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import py4dgeo\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import laspy\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely.geometry import Point, Polygon\n",
    "from scipy.spatial import KDTree\n",
    "import matplotlib.pyplot as plt\n",
    "import pdal\n",
    "import pooch\n",
    "\n",
    "# Download data from zenodo and set path to point cloud folder\n",
    "p = pooch.Pooch(base_url=\"doi:10.5281/zenodo.10003574/\", path=pooch.os_cache(\"py4dgeo\"))\n",
    "p.load_registry_from_doi()\n",
    "p.fetch(\"module3.zip\", processor=pooch.Unzip())\n",
    "pc_dir = os.path.join(p.path, \"module3.zip.unzip/module3/ahk\")\n",
    "\n",
    "# list of point clouds (time series)\n",
    "pc_list = os.listdir(pc_dir)\n",
    "pc_list[:5]  # print the first elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf87688",
   "metadata": {},
   "source": [
    "Specify the path to the data and names of input files, and read in the point cloud data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c69b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive the file paths and read in the data\n",
    "pc_file_2020 = f\"{pc_dir}/ahk_2020_uls.laz\"\n",
    "pc_file_2021 = f\"{pc_dir}/ahk_2021_uls.laz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394e72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to read las file\n",
    "def read_las(infile, get_attributes=False, use_every=1):\n",
    "    \"\"\"\n",
    "    Function to read coordinates and optionally attribute information of point cloud data from las/laz file.\n",
    "\n",
    "    :param infile: specification of input file (format: las or laz)\n",
    "    :param get_attributes: if True, will return all attributes in file, otherwise will only return coordinates (default is False)\n",
    "    :param use_every: value specifies every n-th point to use from input, i.e. simple subsampling (default is 1, i.e. returning every point)\n",
    "    :return: Array of point coordinates of shape (N,3) with N number of points in input file (or subsampled by 'use_every')\n",
    "    \"\"\"\n",
    "\n",
    "    # read the file using the laspy read function\n",
    "    indata = laspy.read(infile)\n",
    "\n",
    "    # get the coordinates (XYZ) and stack them in a 3D array\n",
    "    coords = np.vstack((indata.x, indata.y, indata.z)).transpose()\n",
    "\n",
    "    # subsample the point cloud, if use_every = 1 will remain the full point cloud data\n",
    "    coords = coords[::use_every, :]\n",
    "\n",
    "    # read attributes if get_attributes is set to True\n",
    "    if get_attributes == True:\n",
    "        # get all attribute names in the las file as list\n",
    "        las_fields = list(indata.points.point_format.dimension_names)\n",
    "\n",
    "        # create a dictionary to store attributes\n",
    "        attributes = {}\n",
    "\n",
    "        # loop over all available fields in the las point cloud data\n",
    "        for las_field in las_fields[\n",
    "            3:\n",
    "        ]:  # skip the first three fields, which contain coordinate information (X,Y,Z)\n",
    "            attribute = np.array(\n",
    "                indata.points[las_field]\n",
    "            )  # transpose shape to (N,1) to fit coordinates array\n",
    "            if np.sum(attribute) == 0:  # if field contains only 0, it is empty\n",
    "                continue\n",
    "            # add the attribute to the dictionary with the name (las_field) as key\n",
    "            attributes[las_field] = attribute[\n",
    "                ::use_every\n",
    "            ]  # subsample by use_every, corresponding to point coordinates\n",
    "\n",
    "        # return coordinates and attribute data\n",
    "        return (coords, attributes)\n",
    "\n",
    "    else:  # get_attributes == False\n",
    "        return coords  # return coordinates only\n",
    "\n",
    "\n",
    "# read the point clouds into numpy arrays\n",
    "pc_2020 = read_las(pc_file_2020)\n",
    "pc_2021 = read_las(pc_file_2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a1295",
   "metadata": {},
   "source": [
    "## Assessment of alignment uncertainty\n",
    "The epochs are georeferenced, i.e., multitemporal point clouds are registered in a common coordinate reference frame. The point clouds have further been fine aligned using an ICP method on stable parts outside the rock glacier area. You may assume that the best possible alignment between the multitemporal data has been achieved.\n",
    "\n",
    "For change analysis, it is important to assess the uncertainty of the point cloud alignment for information on the minimum detectable change. Do this, by cutting out some stable rock surfaces outside the rock glacier and checking the cloud-to-cloud distances for these subsets. You may cut out point cloud subsets manually in CloudCompare, or define polygons (e.g., in QGIS) to extract the parts from the full point cloud in the Python workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d9db8",
   "metadata": {},
   "source": [
    "We check the alignment uncertainty by cutting out some stable rock surfaces outside the rock glacier. In this solution, we have derived polygons of stable parts for one point cloud epoch (`ahk_2021_uls_stableparts.shp`). Using this, we can extract the points within stable parts (2D query) and subsequently derive the point cloud distances between epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify input shapefile\n",
    "shp_file = f\"{pc_dir}/ahk_2021_uls_stableparts.shp\"\n",
    "\n",
    "# read the shapefile\n",
    "datasource = gpd.read_file(shp_file)\n",
    "polygons = gpd.GeoSeries(datasource[\"geometry\"])\n",
    "\n",
    "# to speed up the polygon selection, we make a coarse subsetting based on the XY bounding box of the polygons\n",
    "miny = polygons.bounds[\"miny\"].values[0]\n",
    "maxy = polygons.bounds[\"maxy\"].values[0]\n",
    "minx = polygons.bounds[\"minx\"].values[0]\n",
    "maxx = polygons.bounds[\"maxx\"].values[0]\n",
    "pc_2020_subs = pc_2020[\n",
    "    (pc_2020[:, 1] >= miny)\n",
    "    & (pc_2020[:, 1] <= maxy)\n",
    "    & (pc_2020[:, 0] >= minx)\n",
    "    & (pc_2020[:, 0] <= maxx)\n",
    "]\n",
    "pc_2021_subs = pc_2021[\n",
    "    (pc_2021[:, 1] >= miny)\n",
    "    & (pc_2021[:, 1] <= maxy)\n",
    "    & (pc_2021[:, 0] >= minx)\n",
    "    & (pc_2021[:, 0] <= maxx)\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Remaining points: {pc_2020_subs.shape[0]}/{pc_2020.shape[0]} for 2020; {pc_2021_subs.shape[0]}/{pc_2021.shape[0]} for 2021\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a6774",
   "metadata": {},
   "source": [
    "In the next part, we check each remaining point for its position in the polygon/stable area. You may take a coffee break while it is running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mask stable parts via polygons for 2020 points...\")\n",
    "pts_in_poly_2020_gdf = gpd.GeoDataFrame(\n",
    "    geometry=gpd.points_from_xy(\n",
    "        pc_2020_subs[:, 0], pc_2020_subs[:, 1], pc_2020_subs[:, 2]\n",
    "    ),\n",
    "    crs=\"EPSG:25832\",\n",
    ")\n",
    "pc_stable_2020 = pc_2020_subs[pts_in_poly_2020_gdf.index]\n",
    "\n",
    "print(\"Mask stable parts via polygons for 2021 points...\")\n",
    "pc_2021_gdf = gpd.GeoDataFrame(\n",
    "    geometry=gpd.points_from_xy(\n",
    "        pc_2021_subs[:, 0], pc_2021_subs[:, 1], pc_2021_subs[:, 2]\n",
    "    ),\n",
    "    crs=\"EPSG:25832\",\n",
    ")\n",
    "pts_in_poly_2021_gdf = gpd.sjoin(pc_2021_gdf, datasource, predicate=\"within\")\n",
    "pc_stable_2021 = pc_2021_subs[pts_in_poly_2021_gdf.index]\n",
    "print(\n",
    "    f\"Remaining points: {pc_stable_2020.shape[0]}/{pc_2020.shape[0]} for 2020; {pc_stable_2021.shape[0]}/{pc_2021.shape[0]} for 2021\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582019f6",
   "metadata": {},
   "source": [
    "Create a kd tree and compute the distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 3D kd-tree from the point cloud\n",
    "tree2021 = KDTree(pc_stable_2021[:, :3])\n",
    "\n",
    "# query indices of nearest neighbors of 2020 coordinates in 2021 kd-tree\n",
    "nn_dists = tree2021.query(pc_stable_2020[:, :3], k=1)\n",
    "\n",
    "# obtain distances as first element in tuple returned by query above\n",
    "distances = nn_dists[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1970def",
   "metadata": {},
   "source": [
    "We assess the distances visually, and derive the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7920ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow interactive rotation in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# create a figure with 3D axis\n",
    "fig, ax = plt.subplots(1, 1, subplot_kw={\"projection\": \"3d\"}, figsize=(7, 5))\n",
    "\n",
    "nth = 100\n",
    "\n",
    "# plot the point cloud colored by height (z values)\n",
    "s = ax.scatter(\n",
    "    pc_stable_2020[::nth, 0],\n",
    "    pc_stable_2020[::nth, 1],\n",
    "    pc_stable_2020[::nth, 2],\n",
    "    s=1,\n",
    "    c=distances[::nth],\n",
    ")\n",
    "\n",
    "# label axes and add title\n",
    "ax.set_xlabel(\"X [m]\")\n",
    "ax.set_ylabel(\"Y [m]\")\n",
    "ax.set_zlabel(\"Z [m]\")\n",
    "\n",
    "# set initial view of 3D plot\n",
    "ax.view_init(elev=40.0, azim=130.0)\n",
    "\n",
    "# add a colorbar\n",
    "fig.colorbar(s, shrink=0.5, aspect=10, label=\"NN distance [m]\", ax=ax, pad=0.2)\n",
    "\n",
    "# show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3629de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print statistics of nn distances\n",
    "print(f\"Median distances: {np.median(distances):.3f} m\")\n",
    "print(f\"Std. dev. of distances: {np.std(distances):.3f} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166b913",
   "metadata": {},
   "source": [
    "The distances are quite large according to the statistics. We would assume lower values for the (visual) quality of the co-registration and the measurement accuracy of UAV laser scanning in this scene. An explanation for the large values could be the influence of point sampling, so you see here an effect of the drawbacks in simple cloud-to-cloud (C2C) distance computation. For a potentially more robust result, we derive the M3C2 distances in these stable parts (for a detailed solution how to establish the M3C2 change analysis, see the remainder of the solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cdf447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the stable parts as py4dgeo Epochs\n",
    "epoch2020_stable = py4dgeo.epoch.as_epoch(pc_stable_2020)\n",
    "epoch2021_stable = py4dgeo.epoch.as_epoch(pc_stable_2021)\n",
    "\n",
    "# instantiate the M3C2 algorithm object, using all points of 2020 as corepoints (no subsampling)\n",
    "m3c2 = py4dgeo.M3C2(\n",
    "    epochs=(epoch2020_stable, epoch2021_stable),\n",
    "    corepoints=epoch2020_stable.cloud[::],\n",
    "    normal_radii=(0.5,),\n",
    "    cyl_radii=(0.5,),\n",
    "    max_distance=(5.0),\n",
    "    registration_error=(0.0),\n",
    ")\n",
    "\n",
    "# run the distance computation\n",
    "m3c2_distances_stableparts, uncertainties_stableparts = m3c2.run()\n",
    "\n",
    "# print statistics of nn distances\n",
    "print(f\"Median M3C2 distances: {np.nanmedian(m3c2_distances_stableparts):.3f} m\")\n",
    "print(f\"Std. dev. of M3C2 distances: {np.nanstd(m3c2_distances_stableparts):.3f} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dbfd91",
   "metadata": {},
   "source": [
    "As the M3C2 distances provide a more robust etimate of point distances in the stable parts, we use their standard deviation as measure of alignment accuracy in our change analysis. This measure we can later on use as registration error for deriving the level of detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06bab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_2020_2021 = np.nanstd(m3c2_distances_stableparts)\n",
    "print(\n",
    "    f\"Registration error based on point distances in stable parts is {reg_2020_2021:.3f} m.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68894990",
   "metadata": {},
   "source": [
    "This value already provides as an estimate of change magnitudes which can be confidently derived from point cloud comparison of these two epochs. In the M3C2 distance calculation, we will derive the per-point level of detection considering the local (i.e. spatially variable) roughness and point density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec53166",
   "metadata": {},
   "source": [
    "By the way, we can free up some working memory by removing the point cloud arrays here. In the following, we continue with `py4dgeo` and therefore load the data into (new) `Epoch`objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1908bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_2020 = None\n",
    "pc_2021 = None\n",
    "pc_2020_subs = None\n",
    "pc_2021_subs = None\n",
    "pc_stable_2020 = None\n",
    "pc_stable_2021 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3400fba8",
   "metadata": {},
   "source": [
    "## 3D change analysis via point cloud distance computation\n",
    "\n",
    "Calculate the distance between point clouds of the two epochs using the M3C2 algorithm ([Lague et al., 2013](https://doi.org/10.1016/j.isprsjprs.2013.04.009)). \n",
    "\n",
    "Think about a suitable parametrization:\n",
    "* Normal scale D: diameter of point neighborhood to use for normal vector computation. *Hint*: Aim for overall, larger-scale surface change, e.g. due to rock glacier creep or heave/thaw (instead of individual boulders).\n",
    "* Projection scale d: diameter of cylindrical point neighborhood to obtain average position.\n",
    "* Maximum cylinder depth: Maximum distance along normal at which to obtain distances to the other point cloud.\n",
    "* Preferred normal orientation.\n",
    "* Registration error ~ alignment accuracy\n",
    "\n",
    "Consider using the [py4dgeo documentation](https://py4dgeo.readthedocs.io/en/latest/intro.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e000f438",
   "metadata": {},
   "source": [
    "To perform the M3C2 change analyis with py4dgeo, we need to read the data into `Epoch` objects. This can be done from the `Nx3` array of coordinates using the function `py4dgeo.epoch.as_epoch()`, or directly from file using `py4dgeo.read_from_las()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec899db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load epochs into py4dgeo objects\n",
    "epoch2020 = py4dgeo.read_from_las(pc_file_2020)\n",
    "epoch2021 = py4dgeo.read_from_las(pc_file_2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ce160",
   "metadata": {},
   "source": [
    "As core points, we use a subsampling of coordinates in the epoch of 2020 (which will be the reference for deriving M3C2 distances to the later epoch of 2021), e.g. using every 100th point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b6f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use every nth point as core point for distance calculation\n",
    "corepoints = epoch2020.cloud[::100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d8ceaa",
   "metadata": {},
   "source": [
    "We parametrize the M3C2 algorithm object according to the parameters in [Zahs et al., 2019](#references), and use the registration error we derived above. The default normal orientation in `py4dgeo` is upwards (``), so we do not need to adapt/set this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256a187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate and parametrize the M3C2 algorithm object\n",
    "m3c2 = py4dgeo.M3C2(\n",
    "    epochs=(epoch2020, epoch2021),\n",
    "    corepoints=corepoints,\n",
    "    normal_radii=(4.0, 0.5, 6.0),\n",
    "    cyl_radii=(0.5,),\n",
    "    max_distance=(15.0),\n",
    "    registration_error=(reg_2020_2021),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea461f",
   "metadata": {},
   "source": [
    "Now the analysis can be run - and another small break would be suitable for the waiting time ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34415dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3c2_distances, uncertainties = m3c2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b401e6c1",
   "metadata": {},
   "source": [
    "All change information (distances and uncertainties) are now contained in the returned objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8dc256",
   "metadata": {},
   "source": [
    "## Change representation and assessment of results\n",
    "\n",
    "Visualize the obtained point cloud distances and corresponding information layers, such as the level of detection and the normal vectors representing the direction of changes.\n",
    "\n",
    "Prepare the result assessment for interpretation by analysts. For example, you may rasterize the different layers and create a map, e.g., of the derived surface changes in GIS software. Tip: Use the Web Map Service (WMS) of Tyrol as a basemap: [https://gis.tirol.gv.at/arcgis/services/Service_Public/orthofoto/MapServer/WMSServer](https://gis.tirol.gv.at/arcgis/services/Service_Public/orthofoto/MapServer/WMSServer).\n",
    "\n",
    "\n",
    "What are the different properties of visible changes and to which types of surface activity would you attribute them? Think of heave and subsidence processes, individual boulder movement, and rock glacier creep (cf. [Zahs et al., 2019](#references))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366b6129",
   "metadata": {},
   "source": [
    "Using the level of detection computed by the M3C2, we derive for each point if the quantified change is significant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a binary mask of significant change\n",
    "change_sign = np.where(abs(m3c2_distances) > uncertainties[\"lodetection\"], True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4951ff",
   "metadata": {},
   "source": [
    "Now we create a composite figure of the change values (distances), change directions (vertical component), level of detection and significant change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59792d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the figure\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 14), subplot_kw={\"projection\": \"3d\"})\n",
    "(ax1, ax2), (ax3, ax4) = axs\n",
    "\n",
    "# plot the distances\n",
    "d = ax1.scatter(\n",
    "    corepoints[:, 0],\n",
    "    corepoints[:, 1],\n",
    "    corepoints[:, 2],\n",
    "    c=m3c2_distances,\n",
    "    cmap=\"seismic_r\",\n",
    "    vmin=-0.15,\n",
    "    vmax=0.35,\n",
    "    s=1,\n",
    ")\n",
    "plt.colorbar(d, format=(\"%.2f\"), label=\"Distance [m]\", ax=ax1, shrink=0.5, pad=0.15)\n",
    "\n",
    "# plot the directions\n",
    "directions = m3c2.directions()\n",
    "dz = ax2.scatter(\n",
    "    corepoints[:, 0],\n",
    "    corepoints[:, 1],\n",
    "    corepoints[:, 2],\n",
    "    c=directions[:, 2],\n",
    "    cmap=\"cool\",\n",
    "    s=1,\n",
    ")\n",
    "plt.colorbar(dz, format=(\"%.2f\"), label=\"Normal Z [0,1]\", ax=ax2, shrink=0.5, pad=0.15)\n",
    "\n",
    "# plot the level of detection values\n",
    "l = ax3.scatter(\n",
    "    corepoints[:, 0],\n",
    "    corepoints[:, 1],\n",
    "    corepoints[:, 2],\n",
    "    c=uncertainties[\"lodetection\"],\n",
    "    cmap=\"viridis\",\n",
    "    vmax=0.15,\n",
    "    s=1,\n",
    ")\n",
    "plt.colorbar(\n",
    "    l,\n",
    "    format=(\"%.2f\"),\n",
    "    label=\"Level of detection [m]\",\n",
    "    ax=ax3,\n",
    "    extend=\"max\",\n",
    "    shrink=0.5,\n",
    "    pad=0.15,\n",
    ")\n",
    "\n",
    "# plot the significant change values (boolean)\n",
    "# ax4.scatter(corepoints[~change_sign][:,0], corepoints[~change_sign][:,1], corepoints[~change_sign][:,2], label='Not significant change', c='blue', s=1) # if added, visibility of significant change areas is poor\n",
    "ax4.scatter(\n",
    "    corepoints[change_sign][:, 0],\n",
    "    corepoints[change_sign][:, 1],\n",
    "    corepoints[change_sign][:, 2],\n",
    "    label=\"Significant change\",\n",
    "    c=\"red\",\n",
    "    s=1,\n",
    ")\n",
    "ax4.legend()\n",
    "\n",
    "# add plot elements\n",
    "for ax_set in axs:\n",
    "    for ax in ax_set:\n",
    "        ax.set_xlabel(\"Easting [m]\")\n",
    "        ax.set_ylabel(\"Northing [m]\")\n",
    "        ax.set_aspect(\"equal\")\n",
    "        ax.view_init(elev=30.0, azim=120.0)\n",
    "\n",
    "plt.axis(\"equal\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d982d62",
   "metadata": {},
   "source": [
    "For further analysis and visualization in external software, we store the result in one point cloud as laz file, adding all the attributes of the change analysis result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598eac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path for the las file\n",
    "pc_m3c2 = f\"{data_path}/ahk_2020_2021_m3c2.laz\"\n",
    "\n",
    "# create a dictionary of attributes to store with the point cloud\n",
    "attr = {\n",
    "    \"m3c2_distance\": m3c2_distances,\n",
    "    \"level_of_detection\": uncertainties[\"lodetection\"],\n",
    "    \"significant_change\": change_sign.astype(int),\n",
    "    \"NormalX\": directions[:, 0],\n",
    "    \"NormalY\": directions[:, 1],\n",
    "    \"NormalZ\": directions[:, 2],\n",
    "}\n",
    "\n",
    "\n",
    "# define function to write las file\n",
    "def write_las(outpoints, outfilepath, attribute_dict={}, correct_wkt_entry=True):\n",
    "    \"\"\"\n",
    "    :param outpoints: 3D array of points to be written to output file\n",
    "    :param outfilepath: specification of output file (format: las or laz)\n",
    "    :param attribute_dict: dictionary of attributes (key: name of attribute; value: 1D array of attribute values in order of points in 'outpoints'); if not specified, dictionary is empty and nothing is added\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    # create a header for new las file\n",
    "    hdr = laspy.LasHeader(version=\"1.4\", point_format=6)\n",
    "\n",
    "    # set the coordinate resolutions and offset in the header\n",
    "    hdr.x_scale = 0.00025\n",
    "    hdr.y_scale = 0.00025\n",
    "    hdr.z_scale = 0.00025\n",
    "    mean_extent = np.mean(outpoints, axis=0)\n",
    "    hdr.x_offset = int(mean_extent[0])\n",
    "    hdr.y_offset = int(mean_extent[1])\n",
    "    hdr.z_offset = int(mean_extent[2])\n",
    "\n",
    "    # create the las data\n",
    "    las = laspy.LasData(hdr)\n",
    "\n",
    "    # write coordinates into las data\n",
    "    las.x = outpoints[:, 0]\n",
    "    las.y = outpoints[:, 1]\n",
    "    las.z = outpoints[:, 2]\n",
    "\n",
    "    # add all dictionary entries to las data (if available)\n",
    "    for key, vals in attribute_dict.items():\n",
    "        if not key in las:\n",
    "            las.add_extra_dim(laspy.ExtraBytesParams(name=key, type=type(vals[0])))\n",
    "        las[key] = vals\n",
    "\n",
    "    # write las file\n",
    "    las.write(outfilepath)\n",
    "\n",
    "    # this is required because alobal encoding WKT flag must be set for point format 6 - 10 since las 1.4\n",
    "    # otherwise programs such as pdal will not be able to read the file\n",
    "    if correct_wkt_entry:\n",
    "        filename = outfilepath\n",
    "        f = open(filename, \"rb+\")\n",
    "        f.seek(6)\n",
    "        f.write(bytes([17, 0, 0, 0]))\n",
    "        f.close()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# use las write function to write file\n",
    "pcfuncs.write_las(corepoints, pc_m3c2, attribute_dict=attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84fafc2",
   "metadata": {},
   "source": [
    "To create raster layers for further analysis and visualization in a GIS, we use [PDAL](https://pdal.io/en/latest/stages/writers.gdal.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d5eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing backslashes by forward slashes is required for some Windows paths\n",
    "pc_m3c2 = pc_m3c2.replace(\"\\\\\", \"/\")\n",
    "\n",
    "# define the raster file names based on the input file name\n",
    "raster_distance = pc_m3c2.replace(\".laz\", \"_distances.tif\")\n",
    "raster_lodet = pc_m3c2.replace(\".laz\", \"_lodetection.tif\")\n",
    "\n",
    "# define the pdal pipeline as json, \"dimension\" allows us to define the attribute to be rasterized (default: \"Z\" value / elevation)\n",
    "json_rast = \"\"\"[\n",
    "    \"%s\",\n",
    "    {\n",
    "        \"type\":\"writers.gdal\",\n",
    "        \"filename\": \"%s\",\n",
    "        \"output_type\":\"min\",\n",
    "        \"gdaldriver\":\"GTiff\",\n",
    "        \"resolution\":1.0,\n",
    "        \"dimension\":\"%s\",\n",
    "        \"window_size\":8\n",
    "    }\n",
    "]\"\"\"\n",
    "\n",
    "# execute the pipeline for the distance raster\n",
    "json_dist = json_rast % (pc_m3c2, raster_distance, \"m3c2_distance\")\n",
    "pipeline = pdal.Pipeline(json_dist)\n",
    "exe = pipeline.execute()\n",
    "\n",
    "# execute the pipeline for the lodetection raster\n",
    "json_lodet = json_rast % (pc_m3c2, raster_lodet, \"level_of_detection\")\n",
    "pipeline = pdal.Pipeline(json_lodet)\n",
    "exe = pipeline.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d25a9e5",
   "metadata": {},
   "source": [
    "You can now create useful raster-based map visualizations as well, e.g. in QGIS. In the corresponding [solution video](https://www.youtube.com/embed/6TTyI-SuDlw) you can see how this is done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc2b0b8",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Lague, D., Brodu, N., & Leroux, J. (2013). Accurate 3D comparison of complex topography with terrestrial laser scanner: Application to the Rangitikei canyon (N-Z). ISPRS Journal of Photogrammetry and Remote Sensing, 82, pp. 10-26. doi: [10.1016/j.isprsjprs.2013.04.009](https://doi.org/10.1016/j.isprsjprs.2013.04.009).\n",
    "\n",
    "* Zahs, V., Hämmerle, M., Anders, K., Hecht, S., Sailer, R., Rutzinger, M., Williams, J. G., & Höfle, B. (2019). Multi-temporal 3D point cloud-based quantification and analysis of geomorphological activity at an alpine rock glacier using airborne and terrestrial LiDAR. Permafrost and Periglacial Processes, 30 (3), pp. 222-238. doi: [10.1002/ppp.2004](https://doi.org/10.1002/ppp.2004)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "bca379f2a991149e96af4e280266aaefc3952de62153de79b5ea1b1512730db6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
